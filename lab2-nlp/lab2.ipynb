{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8530d245-7473-4431-b91b-736ddd9593e6",
   "metadata": {},
   "source": [
    "### Plan:\n",
    "1. **Get tokens** for positive and negative tweets (by `token` in this context we mean `word`).\n",
    "2. **Lemmatize** them (convert to base word forms). For that we will use a Part-of-Speech tagger.\n",
    "3. **Clean'em up** (remove mentions, URLs, stop words).\n",
    "4. **Prepare models** for the classifier, based on cleaned-up tokens.\n",
    "5. **Run the Naive Bayes classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dae761c-bd27-4d32-be19-f07965ced307",
   "metadata": {},
   "source": [
    "First, download necessary prepared samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42819310-88ea-463b-b6a8-cd88a312c4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2ee03cc-4142-4362-85e6-508597ad4f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/user/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca88fbc7-e208-46cc-b748-eeb3a9191fe6",
   "metadata": {},
   "source": [
    "Get some sample positive/negative tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727ac170-6412-41e6-b6c6-1038906ffbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import twitter_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b8c01a-fffb-42dd-bfe5-c0cf24bb7c38",
   "metadata": {},
   "source": [
    "We can either get the actual string content of those tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25b8a199-3b39-4635-b658-375493490988",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae328604-e25e-4abf-a6b8-5d664f1438c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@groovinshawn they are rechargeable and it normally comes with a charger when u buy it :)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_tweets[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d184a10-a0be-477b-8e9a-36749a32730a",
   "metadata": {},
   "source": [
    "Or we can get a list of tokens using [tokenized method](https://www.nltk.org/howto/twitter.html) on `twitter_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd4b52e-d58e-4e5d-9275-0097185fbf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff367ad-5a64-4ce9-b81b-e7c9bc9103a1",
   "metadata": {},
   "source": [
    "Now let's setup a Part-of-Speech tagger.  Download a perceptron tagger that will be used by the PoS tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "313b4107-4729-43c4-bb72-31be6498e4ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/user/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a170912-6686-4b4d-9e09-a6b56cb80e68",
   "metadata": {},
   "source": [
    "Import Part-of-Speech tagger that will be used for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c64174c-0358-474c-a206-8f0038f7fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea756f9e-c15b-402c-9f80-bad6de2c25f3",
   "metadata": {},
   "source": [
    "Check how it works. Note that it returns tuples, where second element is a Part-of-Speech identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a962113-b87c-40f9-896c-a2799447d4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('@groovinshawn', 'NN'),\n",
       " ('they', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('rechargeable', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('it', 'PRP'),\n",
       " ('normally', 'RB'),\n",
       " ('comes', 'VBZ'),\n",
       " ('with', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('charger', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('u', 'JJ'),\n",
       " ('buy', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " (':)', 'JJ')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(tweet_tokens[50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7f4c99-cb9a-40b1-a617-fe172051fa05",
   "metadata": {},
   "source": [
    "Let's write a function that will lemmatize twitter tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70d698-6050-4bd9-b365-66ef10e6de66",
   "metadata": {},
   "source": [
    "For that, let's first fetch a WordNet resource. WordNet is a semantically-oriented dictionary of English - check chapter 2.5 of the NLTK book. In online version, this is part 5 [here](https://www.nltk.org/book/ch02.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "560e3e7b-a172-471c-8052-6dabbae85813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04a3d4c-9080-4dbc-a077-2a6e234e03c8",
   "metadata": {},
   "source": [
    "Now fetch PoS tokens so that they can be passed to `WordNetLemmatizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a44c22c2-e6d3-47f4-a294-c3b78509d842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "tokens = tweet_tokens[50]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_sentence = []\n",
    "\n",
    "for word, tag in pos_tag(tokens):\n",
    "    if tag.startswith('NN'):\n",
    "        pos = 'n'\n",
    "    elif tag.startswith('VB'):\n",
    "        pos = 'v'\n",
    "    else:\n",
    "        pos = 'a'\n",
    "    lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba42f25-399f-40bf-a7c0-d0ff912a2f3f",
   "metadata": {},
   "source": [
    "Note that it converts words to their base forms ('are' -> 'be', 'comes' -> 'come').\n",
    "\n",
    "Now we can proceed to processing. \n",
    "During processing, we will perform cleanup:\n",
    "  - remove URLs and mentions using regexes\n",
    "  - after lemmatization, remove *stopwords*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "478174c9-8375-4722-88f7-8b551ed5ea59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e47fe6-b896-4583-b971-50764e7484f9",
   "metadata": {},
   "source": [
    "What are these stopwords? Let's see some."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac4a45d9-327b-4d06-beb1-259809529c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "i\n",
      "me\n",
      "my\n",
      "myself\n",
      "we\n",
      "our\n",
      "ours\n",
      "ourselves\n",
      "you\n",
      "you're\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "for i in range(10):\n",
    "    print(stop_words[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc3201-6336-4a39-9628-bf76bf612b32",
   "metadata": {},
   "source": [
    "Here comes the `process_tokens` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2862fbeb-2a75-4b09-997d-1767c07e0541",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def process_tokens(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        if (re.search(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', token) or \n",
    "            re.search(r'(@[A-Za-z0-9_]+)', token) or \n",
    "            re.search(r'#.*', token)):\n",
    "            continue\n",
    "\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "   \n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c7c49-46cf-4537-9fe4-750da9c38304",
   "metadata": {},
   "source": [
    "Let's test `process_tokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dafb0eb2-44c4-4d24-8bc8-2d97ed56e5f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "print(\"Before:\", tweet_tokens[50])\n",
    "print(\"After:\", process_tokens(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7880f61-72f3-4f61-b632-95df93e615dc",
   "metadata": {},
   "source": [
    "Run `process_tokens` on all positive/negative tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a4f99b-39e4-4764-83ab-8ad09dab51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92716ef9-e8d1-43d3-bc94-621362601f60",
   "metadata": {},
   "source": [
    "Let's see how did the processing go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d90aa3b-6606-4492-901c-6f26519e926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "['dang', 'rad', ':d']\n"
     ]
    }
   ],
   "source": [
    "print(positive_tweet_tokens[500])\n",
    "print(positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a7964-ae39-42a5-b8da-a4a9ba9912da",
   "metadata": {},
   "source": [
    "Let's see what is most common there. Add a helper function `get_all_words`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "400ab380-4422-48bb-a513-27ca12d3ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words(cleaned_tokens_list):\n",
    "    return [w for tokens in cleaned_tokens_list for w in tokens]\n",
    "\n",
    "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515b74f0-34db-4295-b453-cb05dda8aefa",
   "metadata": {},
   "source": [
    "Perform frequency analysis using `FreqDist`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05fa8805-6013-496e-8673-51308d607e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(freq_dist_pos.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055ad9c7-30dd-4f83-a307-7839fe03757e",
   "metadata": {},
   "source": [
    "Fine. Now we'll convert these to a data structure usable for NLTK's naive Bayes classifier ([docs here](https://www.nltk.org/_modules/nltk/classify/naivebayes.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "790f1f82-d35d-45d9-90b3-52682501eb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['top', 'engage', 'member', 'community', 'week', ':)']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tweet_tokens for tweet_tokens in positive_cleaned_tokens_list][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e171ca9-67a1-424a-92c8-88cb4e47d76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_dict(tokens):\n",
    "    return dict([token, True] for token in tokens)\n",
    "    \n",
    "def get_tweets_for_model(cleaned_tokens_list):   \n",
    "    return [get_token_dict(tweet_tokens) for tweet_tokens in cleaned_tokens_list]\n",
    "\n",
    "positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d61171-054b-468f-bc92-e57494690f9d",
   "metadata": {},
   "source": [
    "Create two datasets for positive and negative tweets. Use 7000/3000 split for train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18964303-5c20-4a88-bcd1-df5f7e703aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                     for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                     for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "dataset = positive_dataset + negative_dataset\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "train_data = dataset[:7000]\n",
    "test_data = dataset[7000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb231b7-263f-45bb-abca-909bab161700",
   "metadata": {},
   "source": [
    "Finally we use the nltk's NaiveBayesClassifier on the training data we've just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a928308c-c221-4047-a3ff-dd56b2686c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without hashtags is: 0.9956666666666667\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2034.4 : 1.0\n",
      "                      :) = True           Positi : Negati =   1666.8 : 1.0\n",
      "                follower = True           Positi : Negati =     39.9 : 1.0\n",
      "                     sad = True           Negati : Positi =     30.7 : 1.0\n",
      "                  arrive = True           Positi : Negati =     21.9 : 1.0\n",
      "                     bam = True           Positi : Negati =     20.8 : 1.0\n",
      "                    sick = True           Negati : Positi =     19.2 : 1.0\n",
      "              appreciate = True           Positi : Negati =     15.3 : 1.0\n",
      "                    glad = True           Positi : Negati =     14.1 : 1.0\n",
      "               community = True           Positi : Negati =     14.0 : 1.0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "print(\"Accuracy without hashtags is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "print(classifier.show_most_informative_features(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "633e6c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAccuracy with hashtags is: 0.9956666666666667\\nMost Informative Features\\n                      :( = True           Negati : Positi =   2050.2 : 1.0\\n                follower = True           Positi : Negati =     36.2 : 1.0\\n                     sad = True           Negati : Positi =     25.8 : 1.0\\n                     bam = True           Positi : Negati =     22.7 : 1.0\\n                     x15 = True           Negati : Positi =     17.4 : 1.0\\n               community = True           Positi : Negati =     15.2 : 1.0\\n                followed = True           Negati : Positi =     15.2 : 1.0\\n                    luck = True           Positi : Negati =     14.5 : 1.0\\n               goodnight = True           Positi : Negati =     13.9 : 1.0\\n              appreciate = True           Positi : Negati =     13.2 : 1.0\\n              '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Accuracy with hashtags is: 0.9956666666666667\n",
    "Most Informative Features\n",
    "                      :( = True           Negati : Positi =   2050.2 : 1.0\n",
    "                follower = True           Positi : Negati =     36.2 : 1.0\n",
    "                     sad = True           Negati : Positi =     25.8 : 1.0\n",
    "                     bam = True           Positi : Negati =     22.7 : 1.0\n",
    "                     x15 = True           Negati : Positi =     17.4 : 1.0\n",
    "               community = True           Positi : Negati =     15.2 : 1.0\n",
    "                followed = True           Negati : Positi =     15.2 : 1.0\n",
    "                    luck = True           Positi : Negati =     14.5 : 1.0\n",
    "               goodnight = True           Positi : Negati =     13.9 : 1.0\n",
    "              appreciate = True           Positi : Negati =     13.2 : 1.0\n",
    "              \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8218b77-b2b0-4a0b-81bc-9d90cb11b154",
   "metadata": {},
   "source": [
    "Note the Positive:Negative ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb6996-3f21-4bbd-8a71-4e3c2f1eacdf",
   "metadata": {},
   "source": [
    "Let's check some test phrase. First, download punkt sentence tokenizer ([docs here](https://www.nltk.org/api/nltk.tokenize.punkt.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eab87fd4-9909-469b-8c0a-3c4fd3a36199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145ea904-26bb-4f64-9ec0-634ee5f1b7bd",
   "metadata": {},
   "source": [
    "Now we won't rely on `twitter_samples.tokenized`, but rather will use a generic tokenization routine - `word_tokenize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01c24fbd-1dcd-4068-9831-c56c9482a636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "custom_tweet_pos = \"I have a good feeling about this one, it is really great\"\n",
    "\n",
    "custom_tweet_neg = \"I have a good feeling about this one, it is really\"\n",
    "\n",
    "custom_tweet = \"the model doesn't work really GREAT, it just reacts on trigger words. It is expected cause our model doesn't even have trainable parameters.\"\n",
    "\n",
    "custom_tokens = process_tokens(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(get_token_dict(custom_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878bbbbb-7db6-4d20-8d8b-0d8c48ff79e9",
   "metadata": {},
   "source": [
    "Let's package it as a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eea00802-a825-438b-b8ef-5c36519e6fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad :  Negative\n",
      "service is bad :  Negative\n",
      "service is really bad :  Negative\n",
      "service is so terrible :  Negative\n",
      "great service :  Positive\n",
      "they stole my money :  Negative\n"
     ]
    }
   ],
   "source": [
    "def get_sentiment(text):\n",
    "    custom_tokens = process_tokens(word_tokenize(text))\n",
    "    return classifier.classify(get_token_dict(custom_tokens))\n",
    "\n",
    "texts = [\"bad\", \"service is bad\", \"service is really bad\", \"service is so terrible\", \"great service\", \"they stole my money\"]\n",
    "for t in texts:\n",
    "    print(t, \": \", get_sentiment(t))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aff76e-bc19-4c2b-836e-bdddd013faad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ac3bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6cd73078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab(corpus):\n",
    "    vocab = {}\n",
    "    for i, word in enumerate(set(corpus), 1):\n",
    "        vocab[word] = i\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d14650d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10600"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = get_vocab(get_all_words(positive_cleaned_tokens_list) + get_all_words(negative_cleaned_tokens_list))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae55249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ohe(sentence, vocab):\n",
    "    ohe_sentence = [[0 for _ in range(len(vocab) + 1)] for _ in range(len(sentence))]\n",
    "    for i, word in enumerate(sentence):\n",
    "        ohe_sentence[i][vocab.get(word, 0)] = 1\n",
    "        \n",
    "    return ohe_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daea73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_words_representation(sentence, vocab, ohe):\n",
    "    return list(map(sum, list(zip(*ohe(sentence, vocab)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e27f343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_prepared_data = [num_words_representation(sentence, vocab, ohe) for sentence in positive_cleaned_tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f5ff1e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_pos = max(list(map(sum, pos_prepared_data)))\n",
    "max_len_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "691d9e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_prepared_data = [num_words_representation(sentence, vocab, ohe) for sentence in negative_cleaned_tokens_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c8f91d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len_neg = max(list(map(sum, neg_prepared_data)))\n",
    "max_len_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "417a65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_prepared_data = list(zip(pos_prepared_data, [1 for _ in range(len(pos_prepared_data))]))\n",
    "neg_prepared_data = list(zip(neg_prepared_data, [0 for _ in range(len(neg_prepared_data))]))\n",
    "\n",
    "data = pos_prepared_data + neg_prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29ecbf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(zip(*data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c81fa4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(data[0])\n",
    "y = np.array(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b951ffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10601), (10000,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9535aa3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, ..., 0, 0, 0]), 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0], y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e23acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "055c1cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, pos_prepared_data, neg_prepared_data, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6fa5fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1a166fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d93cfe2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.993"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "34c57157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52794484, 0.47205516]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tweet_pos = \"listen last night :) bleed amazing track scotland\"\n",
    "custom_tweet_neg = \"I fucking hate you\"\n",
    "custom_tweet = \"the model doesn't work really GREAT, it just reacts on trigger words. It is expected cause our model doesn't even have trainable parameters.\"\n",
    "\n",
    "custom_tokens = process_tokens(word_tokenize(custom_tweet_neg))\n",
    "\n",
    "test = np.array(num_words_representation(custom_tokens, vocab, ohe))[None, :]\n",
    "\n",
    "\n",
    "# the model's performance is very skeptical. It gives very small weight to bad words and as a result we often get pos predictions.\n",
    "# The accuracy is high though.\n",
    "log_reg.predict_proba(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a99b89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acf66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e96e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
